# -ON-Self_Attention_in_Transformers
This set of Obsidian notes explains self-attention in transformers, covering its advantages, step-by-step architecture, and an example for easy understanding. Each file is linked for easy navigation and structured for clear explanations.
