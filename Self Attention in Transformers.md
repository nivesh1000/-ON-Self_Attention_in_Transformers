# Self Attention
Self-attention is a technique used in transformers to find relationships between different words in a sentence. Unlike traditional models, it does not rely on fixed positions but instead learns which words are important when making predictions.

## Links
- [[Advantages of Self-Attention]]
- [[Self-Attention Architecture]]
- [[Example of Self-Attention]]
